---
title: It's Been A While
date: 2024-06-09
tags: ["genai", "llm", "deep-learning", "anthropic", "embedding"]
---

It's weekend number two of the Deep Learning Specialization with the Purdue University Post Graduate Program in AI and Machine Learning program that I have been participating since January.  This is the seventh and section of the program which won't wrap up until mid-July with only the Capstone Project left to complete to finish out the program.  This week-end was all about Object Detection, Selective Search, Sequential Modeling, Recurrent Neural Networks, Word Embedding, Long Short-Term Memory and Hybrid Modeling.  

It's all a bit like drinking from a firehose, as far as, all the information that is getting through at me, but learning and understanding more about the inter-workings of deep learning neural networks is why I wanted to enroll in this program.  I say all that because my article list this time around is very much heavy on the AI/ML/DL topics.

## **AI/ML/DL/GenAI**

- [A Grand Unified Theory of the AI Hype Cycle](https://blog.glyph.im/2024/05/grand-unified-ai-hype.html?utm_source=tldrai)

  I thought this was a good read and history lesson on the cyclical AI Hype Cycle.  

### **Word Embedding**

- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- [Word2Vec Tutorial - The Skip-Gram Model](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
- [Word Embedding Demo](https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/)
- [word_embeddings_demo.ipynb](https://gist.github.com/fgiobergia/b3a20e097f9b697d0a02fb17685cfd5a)
- [word-embeddings-workshop/Word Embeddings.ipynb](https://github.com/fastai/word-embeddings-workshop/blob/master/Word%20Embeddings.ipynb)
- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)

### **Deep Learning**

- [Practical Deep Learning for Coders](https://course.fast.ai/index.html)
- [Deep Learning for Coders with fastai & PyTorch - The Book](https://course.fast.ai/Resources/book.html)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [LSTM Recurrent Neural Networks â€” How to Teach a Network to Remember the Past](https://towardsdatascience.com/lstm-recurrent-neural-networks-how-to-teach-a-network-to-remember-the-past-55e54c2ff22e) - *Medium Membership Required to View Article*
- [A Visual Guide to Vision Transformers](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html?utm_source=tldrai)

### **GenAI**

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [GitHub: peremartra/Large-Language-Model-Notebooks-Course](https://github.com/peremartra/Large-Language-Model-Notebooks-Course)
- [Spreadsheet Is All You Need - A nanoGPT pipeline packed in a spreadsheet](https://github.com/dabochen/spreadsheet-is-all-you-need?tab=readme-ov-file#spreadsheet-is-all-you-need)
  
  Haven't had a chance to dig into this one yet, but sounds interesting.  From the repo README: *"This is a project that I did to help myself understand how GPT works. It is pretty fun to play with, especially when you are trying to figure out what exactly is going on inside a transformer.  This helped me to visualize the entire structure and the data flow.  All the mechanisms, calculations, matrices inside are fully interactive and configurable."*
  Dabo Chen

- [Mapping the Mind of a Large Language Model](https://www.anthropic.com/research/mapping-mind-language-model?utm_source=tldrai)

### **Prompt Engineering**

- [Ask HN: What is your ChatGPT customization prompt?](https://news.ycombinator.com/item?id=40474716&utm_source=tldrnewsletter)