{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Try with omnivoreql python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from omnivoreql import OmnivoreQL\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Omnivore and OpenAI API details\n",
    "OMNIVORE_API_KEY = os.getenv(\"OMNIVORE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OMNIVORE_USERNAME = os.getenv(\"OMNIVORE_USERNAME\")\n",
    "\n",
    "# Validate API keys\n",
    "if not OMNIVORE_API_KEY or not OPENAI_API_KEY or not OMNIVORE_USERNAME:\n",
    "    logging.error(\"Missing API key(s). Ensure OMNIVORE_API_KEY, OPENAI_API_KEY, and OMNIVORE_USERNAME are set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Initialize Omnivore client\n",
    "omnivore_client = OmnivoreQL(OMNIVORE_API_KEY)\n",
    "\n",
    "def get_my_articles_with_label(label, filename=\"articles.json\"):\n",
    "    \"\"\"Fetch articles with a specific label, caching results to a file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                return json.load(file)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading articles from file: {e}\")\n",
    "\n",
    "    try:\n",
    "        response = omnivore_client.get_articles(\n",
    "            format=\"markdown\",\n",
    "            limit=100,\n",
    "            query=f\"in:inbox AND label:{label}\",\n",
    "            include_content=False\n",
    "        )\n",
    "        articles = [item['node'] for item in response['search']['edges']]\n",
    "\n",
    "        try:\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(articles, file)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing articles to file: {e}\")\n",
    "\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching articles: {e}\")\n",
    "        return []\n",
    "    \n",
    "def openai_request(prompt, max_tokens=150):\n",
    "    \"\"\"Send a request to the OpenAI API.\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except openai.OpenAIError as e:\n",
    "        logging.error(f\"OpenAI error: {e}\")\n",
    "        return \"Error during OpenAI request\"\n",
    "    \n",
    "def summarize_article(content):\n",
    "    \"\"\"Generate a summary for an article using the OpenAI API.\"\"\"\n",
    "    try:\n",
    "        return openai_request(f\"Summarize the following article in three sentences:\\n\\n{content}\", max_tokens=150)\n",
    "    except openai.Error as e:\n",
    "        # Log OpenAI API specific errors\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        # Log other exceptions\n",
    "        logging.error(f\"General error when calling OpenAI API: {e}\")\n",
    "\n",
    "def generate_tags(content, labels):\n",
    "    \"\"\"Generate tags for an article using the OpenAI API.\"\"\"\n",
    "    tags = \"ai, ai-governance, ai-optimization, ai-reliability, ai-security, agi, anthropic, apple, automation, aws, azure, claude, cloud, cloud-computing, cyber-security, data-science, deep-learning, developer-experience, ethical-ai, enterprise-architecture, fin-ops, gcp, generative-ai, github, github-actions, github-repo, gpt-4, jupyter-notebooks, langchain, llm, machine-learning, nlp, openai, personal-development, projects, python, rag, responsible-ai, security\"\n",
    "    prompt = (\n",
    "        f\"Extract and process the tags from the following list: {labels}. \"\n",
    "        f\"Remove all single quotes and brackets, resulting in a comma-separated list of tags that only contains letters, numbers, and dashes. \"\n",
    "        f\"From this list, determine how many tags are present and subtract that number from five to determine how many additional tags are needed. \"\n",
    "        f\"From the following list of available tags: {tags}, select additional tags that match the content of the article below, ensuring the total number of tags is five. \"\n",
    "        f\"If there are not enough matching tags from the provided list, create new tags following these rules: \"\n",
    "        f\"1. Use no more than three words, \"\n",
    "        f\"2. Use only lowercase letters, \"\n",
    "        f\"3. Replace spaces with dashes. \"\n",
    "        f\"Return exactly five tags as a comma-separated list with no additional text or explanation. \"\n",
    "        f\"Here is the article content:\\n\\n{content}\"\n",
    "    )\n",
    "    try:\n",
    "        return openai_request(prompt, max_tokens=150)\n",
    "    except openai.Error as e:\n",
    "        # Log OpenAI API specific errors\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        # Log other exceptions\n",
    "        logging.error(f\"General error when calling OpenAI API: {e}\")\n",
    "\n",
    "\n",
    "def get_web_page_content(slug):\n",
    "    \"\"\"Retrieve the web page content for a given article slug.\"\"\"\n",
    "    try:\n",
    "        response = omnivore_client.get_article(\n",
    "            username=OMNIVORE_USERNAME,\n",
    "            slug=slug,\n",
    "            format=\"markdown\"\n",
    "        )\n",
    "        return response['article']['article']['content']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching article content: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def process_articles(articles, exclude_label):\n",
    "    \"\"\"Process articles to generate summaries and tags.\"\"\"\n",
    "    processed_articles = []\n",
    "    if os.path.exists('processed_articles.csv'):\n",
    "        processed_articles = pd.read_csv('processed_articles.csv').to_dict('records')\n",
    "    else:\n",
    "        for article in articles:\n",
    "            title = article.get(\"title\", \"Untitled\")\n",
    "            site_name = article.get(\"siteName\", \"Unknown\")\n",
    "            description = article.get(\"description\", \"\")\n",
    "            link = article.get(\"url\", \"\")\n",
    "            slug = article.get(\"slug\", \"\")\n",
    "            labels = [label['name'] for label in article['labels'] if label['name'] != exclude_label]\n",
    "            content = get_web_page_content(slug)\n",
    "            if content:\n",
    "                logging.info(f\"Processing article: {title}\")\n",
    "                summary = summarize_article(content)\n",
    "                if \"no content\" in summary.lower():\n",
    "                    tags = ['None']\n",
    "                else:\n",
    "                    tags = generate_tags(content, labels).split(\",\")\n",
    "                    tags = [tag.strip() for tag in tags]\n",
    "                processed_articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"tags\": tags,\n",
    "                    \"site_name\": site_name,\n",
    "                    \"description\": description,\n",
    "                    \"link\": link,\n",
    "                    \"slug\": slug,\n",
    "                    \"content\": content,\n",
    "                    \"summary\": summary\n",
    "                })\n",
    "            else:\n",
    "                logging.warning(f\"Skipping article {title} due to missing content\")\n",
    "        pd.DataFrame(processed_articles).to_csv('processed_articles.csv', index=False)\n",
    "    return processed_articles\n",
    "\n",
    "def create_markdown_content(processed_articles):\n",
    "    \"\"\"Create markdown content from processed articles.\"\"\"\n",
    "    md_content = \"# Articles Summary\\n\\n\"\n",
    "    for article in processed_articles:\n",
    "        # Ensure article['tags'] is a list\n",
    "        tags = article['tags'] if isinstance(article['tags'], list) else [article['tags']]\n",
    "        \n",
    "        # Prepend '#' to each tag and join them with a space\n",
    "        formatted_tags = ', '.join([f\"#{tag}\" for tag in tags])\n",
    "        md_content += (\n",
    "            f\"### [{article['title']}]({article['link']})\\n\\n\"\n",
    "            f\"**Tags:** *{formatted_tags}*\\n\\n\"\n",
    "            f\"**Site Name:** {article['site_name']}\\n\\n\"\n",
    "            f\"**Omnivore Description:** {article['description']}\\n\\n\"\n",
    "            f\"**ChatGPT Summary:** {article['summary']}\\n\\n\"\n",
    "        )\n",
    "    return md_content\n",
    "\n",
    "def write_to_markdown_with_grouping(processed_articles, blog_title, filename):\n",
    "    \"\"\"Write processed articles to a markdown file with front matter and tags.\"\"\"\n",
    "    unique_tags = sorted({tag for article in processed_articles for tag in article['tags'] if tag != 'None'})\n",
    "    formatted_tags = \", \".join([f'\"{tag.strip()}\"' for tag in unique_tags])\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    blog_article_md = create_markdown_content(processed_articles)\n",
    "    front_matter = f\"---\\ntitle: {blog_title}\\ndate: {current_datetime}\\ntags: [{formatted_tags}]\\n---\\n\\n\"\n",
    "    blog_article_md = front_matter + blog_article_md\n",
    "    \n",
    "    return front_matter, blog_article_md\n",
    "\n",
    "def extract_articles(content):\n",
    "    \"\"\"Extract articles from the given content.\"\"\"\n",
    "    try:\n",
    "        articles = re.findall(\n",
    "            r'### \\[(.*?)\\]\\((.*?)\\)\\n\\n\\*\\*Tags:\\*\\* \\*(.*?)\\*\\n\\n\\*\\*Site Name:\\*\\* (.*?)\\n\\n\\*\\*Omnivore Description:\\*\\* (.*?)\\n\\n\\*\\*ChatGPT Summary:\\*\\* (.*?)\\n\\n',\n",
    "            content, re.DOTALL\n",
    "        )\n",
    "        return [{'title': title, 'link': link, 'tags': tags, 'site_name': site_name, 'description': description, 'summary': summary} for title, link, tags, site_name, description, summary in articles]\n",
    "    except openai.Error as e:\n",
    "        # Log OpenAI API specific errors\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        # Log other exceptions\n",
    "        logging.error(f\"General error when calling OpenAI API: {e}\")\n",
    "\n",
    "\n",
    "def generate_categories(articles):\n",
    "    \"\"\"Generate categories for articles using OpenAI API.\"\"\"\n",
    "    prompts = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Group the following articles into 2-5 broad categories based on their tags and content. Provide category names and associate each article with a single category. Format the output as follows:\\n\\n- Category Name\\n  - Article Title\\n  - Article Title\\n\\nThe articles are: \" + \"\\n\".join([f\"Title: {article['title']}, Tags: {article['tags']}\" for article in articles])}\n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=prompts,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except openai.Error as e:\n",
    "        # Log OpenAI API specific errors\n",
    "        logging.error(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        # Log other exceptions\n",
    "        logging.error(f\"General error when calling OpenAI API: {e}\")\n",
    "\n",
    "\n",
    "def parse_categories_response(response):\n",
    "    \"\"\"Parse the categories response from the OpenAI API.\"\"\"\n",
    "    categories = {}\n",
    "    current_category = None\n",
    "    for line in response.split(\"\\n\"):\n",
    "        if line.startswith(\"- \"):\n",
    "            current_category = line[2:].strip()\n",
    "            categories[current_category] = []\n",
    "        elif line.startswith(\"  - \") and current_category:\n",
    "            title = line[4:].strip()\n",
    "            categories[current_category].append(title)\n",
    "    return categories\n",
    "\n",
    "def map_articles_to_categories(articles, categories):\n",
    "    \"\"\"Map articles to categories and extract unique tags.\"\"\"\n",
    "    categorized_articles = {category: {'articles': [], 'tags': set()} for category in categories.keys()}\n",
    "    for article in articles:\n",
    "        for category, titles in categories.items():\n",
    "            if article['title'] in titles:\n",
    "                categorized_articles[category]['articles'].append(article)\n",
    "                for tag in article['tags'].split(','):\n",
    "                    categorized_articles[category]['tags'].add(tag.strip())\n",
    "                break\n",
    "    return categorized_articles\n",
    "\n",
    "def create_table_of_contents(categorized_articles):\n",
    "    \"\"\"Create a table of contents for the categorized articles.\"\"\"\n",
    "    toc = \"# Table of Contents\\n\\n\"\n",
    "    for category in categorized_articles.keys():\n",
    "        toc += f\"- [{category}](#{category.lower().replace(' ', '-')})\\n\"\n",
    "        for article in categorized_articles[category]['articles']:\n",
    "            toc += f\"  - [{article['title']}]({article['link']})\\n\"\n",
    "    # toc += \"\\n-----\\n\\n\"\n",
    "    return toc\n",
    "\n",
    "def create_categorized_markdown_content(categorized_articles):\n",
    "    \"\"\"Create categorized markdown content.\"\"\"\n",
    "    md_content = \"\"\n",
    "    for category, data in categorized_articles.items():\n",
    "        tags_list = ', '.join(sorted(data['tags']))\n",
    "        md_content += f\"-----\\n\\n# {category}\\n\\n**Category Tags:** {tags_list}\\n\\n\"\n",
    "        for article in data['articles']:\n",
    "            md_content += (\n",
    "                f\"### [{article['title']}]({article['link']})\\n\\n\"\n",
    "                f\"**Tags:** *{article['tags']}*\\n\\n\"\n",
    "                f\"**Site Name:** {article['site_name']}\\n\\n\"\n",
    "                f\"**Omnivore Description:** {article['description']}\\n\\n\"\n",
    "                f\"**ChatGPT Summary:** {article['summary']}\\n\\n\"\n",
    "            )\n",
    "        md_content += \"\\n\"\n",
    "    return md_content\n",
    "\n",
    "def main(blog_title, label, filename):\n",
    "    \"\"\"Main function to process and categorize articles, then generate the markdown file.\"\"\"\n",
    "    articles = get_my_articles_with_label(label)\n",
    "    if articles:\n",
    "        processed_articles = process_articles(articles, label)\n",
    "        front_matter, blog_article_md = write_to_markdown_with_grouping(processed_articles, blog_title, filename)\n",
    "    else:\n",
    "        logging.info(\"No articles found with the specified label.\")\n",
    "        return\n",
    "\n",
    "    extracted_articles = extract_articles(blog_article_md)\n",
    "\n",
    "    # Generate categories and categorize articles\n",
    "    categories_response = generate_categories(extracted_articles)\n",
    "    categories = parse_categories_response(categories_response)\n",
    "    categorized_articles = map_articles_to_categories(extracted_articles, categories)\n",
    "\n",
    "    # Create table of contents\n",
    "    toc = create_table_of_contents(categorized_articles)\n",
    "\n",
    "    # Create markdown content\n",
    "    articles_content = create_categorized_markdown_content(categorized_articles)\n",
    "\n",
    "    # Combine TOC and articles content\n",
    "    final_markdown_content = front_matter + toc + articles_content\n",
    "\n",
    "    # Write the final markdown content to a file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(final_markdown_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(blog_title=\"My Blog Title\", label=\"label\", filename=\"index.md\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
